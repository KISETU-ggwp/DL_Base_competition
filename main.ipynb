{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_UcNUR0gZxW"
      },
      "source": [
        "## コンペでの評価が可能になった。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzbHu77Ipmtp"
      },
      "source": [
        "### 下準備\n",
        "- 実行推定時間 15分(A100)\n",
        "\n",
        "- ランタイムにdataをぶん投げる(学習の高速化のために)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n99MNJqvpb4m",
        "outputId": "28672117-856c-43e0-c17b-aac76e0d08e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjjz-7DUp6qI",
        "outputId": "794e1fe0-e7c3-407b-b6fb-c604b1ed46ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files copied successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# コピー元とコピー先のディレクトリを指定\n",
        "src_dir = '/content/drive/MyDrive/DL_base/DL_base_Last/data'\n",
        "dst_dir = '/content/data'\n",
        "\n",
        "# コピー先ディレクトリを作成\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# コピーするファイルのリストを作成\n",
        "files_to_copy = []\n",
        "for root, _, files in os.walk(src_dir):\n",
        "    for file in files:\n",
        "        src_file = os.path.join(root, file)\n",
        "        dst_file = os.path.join(dst_dir, os.path.relpath(src_file, src_dir))\n",
        "        files_to_copy.append((src_file, dst_file))\n",
        "\n",
        "# 並列でファイルをコピーする関数\n",
        "def copy_file(src_dst):\n",
        "    src_file, dst_file = src_dst\n",
        "    dst_file_dir = os.path.dirname(dst_file)\n",
        "    os.makedirs(dst_file_dir, exist_ok=True)\n",
        "    try:\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
        "\n",
        "# ThreadPoolExecutorを使って並列でコピー\n",
        "with ThreadPoolExecutor(max_workers=24) as executor:\n",
        "    executor.map(copy_file, files_to_copy)\n",
        "\n",
        "# ファイルがすべてコピーされたかを確認\n",
        "copied_files = [os.path.join(root, file) for root, _, files in os.walk(dst_dir) for file in files]\n",
        "missing_files = [src for src, dst in files_to_copy if dst not in copied_files]\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"Missing files: {missing_files}\")\n",
        "else:\n",
        "    print(\"All files copied successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLWst9-p7Uo"
      },
      "source": [
        "### VQAの目標値(41%以上)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLKlq20GqEPC"
      },
      "source": [
        "#### 正規表現きちんと決めておく"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BiOSESPNzbn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from statistics import mode\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo6s5QYNqeT9"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7hvKWcTqByj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def process_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 数詞を数字に変換 (zero to twelve)\n",
        "    num_word_to_digit = {\n",
        "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
        "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
        "        'ten': '10', 'eleven': '11', 'twelve': '12'\n",
        "    }\n",
        "    for word, digit in num_word_to_digit.items():\n",
        "        text = re.sub(r'\\b' + word + r'\\b', digit, text)\n",
        "\n",
        "    # 小数点のピリオドを保持しつつ、その他のピリオドを削除\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
        "\n",
        "    # 冠詞の削除\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
        "\n",
        "    # 短縮形の展開\n",
        "    contractions = {\n",
        "        \"n't\": \" not\", \"'s\": \" is\", \"'re\": \" are\", \"'m\": \" am\",\n",
        "        \"'ll\": \" will\", \"'ve\": \" have\", \"'d\": \" would\"\n",
        "    }\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text = text.replace(contraction, expansion)\n",
        "\n",
        "    # 一部の非単語記号を除去 (?, !, \", ')\n",
        "    text = re.sub(r'[?!\"\\']', '', text)\n",
        "\n",
        "    # 一部の非単語記号を分離 ((, ), /, ...)\n",
        "    text = re.sub(r'([()]|/|\\.\\.\\.)', r' \\1 ', text)\n",
        "\n",
        "    # カンマの前後にスペースを追加\n",
        "    text = re.sub(r',', ' , ', text)\n",
        "\n",
        "    # 連続するスペースを1つに変換\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6jNsOI9qNWU"
      },
      "source": [
        "#### Class VQADatasetを定義する(データローダーの作成)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjFp_EJ0qepP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchtext\n",
        "import re\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, df_path, image_dir, transform=None, answer=True, max_length=12, class_mapping=None):\n",
        "        self.df = pd.read_json(df_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.answer = answer\n",
        "        self.max_length = max_length\n",
        "        self.class_mapping = class_mapping\n",
        "\n",
        "        # GloVeの初期化\n",
        "        self.glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
        "\n",
        "        # 質問の語彙を作成\n",
        "        self.question_vocab = self._build_vocab(self.df['question'])\n",
        "\n",
        "        if self.answer and self.class_mapping:\n",
        "            self.idx2answer = {v: k for k, v in self.class_mapping.items()}\n",
        "        elif self.answer:\n",
        "            # class_mappingがない場合は従来の方法で回答の語彙を作成\n",
        "            self.answer_vocab = self._build_vocab([ans['answer'] for answers in self.df['answers'] for ans in answers])\n",
        "            self.idx2answer = {v: k for k, v in self.answer_vocab.items()}\n",
        "\n",
        "    def _build_vocab(self, sentences):\n",
        "        vocab = {'<pad>': 0, '<unk>': 1}\n",
        "        for sentence in sentences:\n",
        "            for word in self.process_text(sentence).split():\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "        return vocab\n",
        "\n",
        "    def process_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
        "        image = self.transform(image) if self.transform else image\n",
        "\n",
        "        question = self.process_text(self.df['question'][idx])\n",
        "        question_tokens = question.split()[:self.max_length]\n",
        "        question_tokens += ['<pad>'] * (self.max_length - len(question_tokens))\n",
        "        question_ids = [self.question_vocab.get(token, self.question_vocab['<unk>']) for token in question_tokens]\n",
        "        question_tensor = torch.tensor(question_ids)\n",
        "\n",
        "        if self.answer and 'answers' in self.df.columns:\n",
        "            answers = [self.process_text(answer['answer']) for answer in self.df['answers'][idx]]\n",
        "            if self.class_mapping:\n",
        "                answer_ids = [self.class_mapping.get(ans, self.class_mapping['unanswerable']) for ans in answers]\n",
        "            else:\n",
        "                answer_ids = [self.answer_vocab.get(ans, self.answer_vocab['<unk>']) for ans in answers]\n",
        "            mode_answer_id = max(set(answer_ids), key=answer_ids.count)\n",
        "            return image, question_tensor, torch.tensor(answer_ids), torch.tensor(mode_answer_id)\n",
        "        else:\n",
        "            return image, question_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_class_mapping(file_path):\n",
        "    class_mapping = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            class_mapping[row['answer']] = int(row['class_id'])\n",
        "    return class_mapping"
      ],
      "metadata": {
        "id": "25Zv7P5HRpe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzzRxglwqmUj"
      },
      "source": [
        "#### 評価指標の実装(BCEの実装 ※松尾研ではのはなし)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqIKbeUtqyHX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def vqa_score(batch_pred: torch.Tensor, batch_answers: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute VQA score for a batch of predictions and ground truth answers.\n",
        "\n",
        "    Args:\n",
        "    batch_pred (torch.Tensor): Predicted answers (batch_size, num_classes)\n",
        "    batch_answers (torch.Tensor): Ground truth answers (batch_size, 10)\n",
        "\n",
        "    Returns:\n",
        "    float: Average VQA score for the batch\n",
        "    \"\"\"\n",
        "    batch_size = batch_pred.size(0)\n",
        "    num_answers = batch_answers.size(1)\n",
        "\n",
        "    # Get the index of the max log-probability\n",
        "    pred = batch_pred.argmax(dim=1)\n",
        "\n",
        "    scores = []\n",
        "    for i in range(batch_size):\n",
        "        answer_count = torch.bincount(batch_answers[i], minlength=batch_pred.size(1))\n",
        "        num_match = answer_count[pred[i]].item()\n",
        "        score = min(num_match / 3, 1)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / batch_size\n",
        "\n",
        "# Example usage\n",
        "# batch_pred = torch.rand(32, 1000)  # (batch_size, num_classes)\n",
        "# batch_answers = torch.randint(0, 1000, (32, 10))  # (batch_size, 10)\n",
        "# score = vqa_score(batch_pred, batch_answers)\n",
        "# print(f\"VQA Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkSnUI11qzYF"
      },
      "source": [
        "#### Modelの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv2S3KAlLWZB"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "class ImageFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg19 = models.vgg19(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(vgg19.features.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        return features.view(x.size(0), 512, -1).permute(0, 2, 1)  # (batch_size, 49, 512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5dFPzhBBWn_"
      },
      "source": [
        "##### GRUエンコーダー"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT8Ae_etq5a8"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv6DVAhgLDYK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8jQLX3oBdd9"
      },
      "source": [
        "##### Co-Attentionモジュール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUQkhVctq5wo"
      },
      "outputs": [],
      "source": [
        "class CoAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.linear_v = nn.Linear(dim, dim)\n",
        "        self.linear_q = nn.Linear(dim, dim)\n",
        "        self.linear_hv = nn.Linear(dim, 1)\n",
        "        self.linear_hq = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, v, q):\n",
        "        v = self.linear_v(v)\n",
        "        q = self.linear_q(q)\n",
        "\n",
        "        hv = torch.tanh(v.unsqueeze(1) + q.unsqueeze(2))\n",
        "        hq = torch.tanh(v.unsqueeze(1) + q.unsqueeze(2))\n",
        "\n",
        "        av = torch.softmax(self.linear_hv(hv).squeeze(-1), dim=2)\n",
        "        aq = torch.softmax(self.linear_hq(hq).squeeze(-1), dim=1)\n",
        "\n",
        "        v_att = torch.bmm(av, v)\n",
        "        q_att = torch.bmm(aq.transpose(1, 2), q)\n",
        "\n",
        "        return v_att, q_att"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1EUcThuBi2h"
      },
      "source": [
        "##### Image Attentionモジュール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCQQh3sYq5tQ"
      },
      "outputs": [],
      "source": [
        "class ImageAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(dim * 2, dim)\n",
        "        self.linear_att = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, v, q):\n",
        "        combined = torch.cat([v, q.unsqueeze(1).repeat(1, v.size(1), 1)], dim=2)\n",
        "        features = torch.tanh(self.linear(combined))\n",
        "        attention = torch.softmax(self.linear_att(features).squeeze(-1), dim=1)\n",
        "        v_att = torch.bmm(attention.unsqueeze(1), v).squeeze(1)\n",
        "        return v_att"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myx1RmqSBpv5"
      },
      "source": [
        "##### 新しいVQAモデル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTiA0pCBmDr"
      },
      "outputs": [],
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_gru_layers=1):\n",
        "        super().__init__()\n",
        "        self.image_extractor = ImageFeatureExtractor()\n",
        "        self.question_encoder = GRUEncoder(vocab_size, embed_size, hidden_size, num_gru_layers)\n",
        "        self.co_attention = CoAttention(hidden_size)\n",
        "        self.image_attention = ImageAttention(hidden_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, question):\n",
        "        img_features = self.image_extractor(image)\n",
        "        que_features = self.question_encoder(question)\n",
        "\n",
        "        v_att, q_att = self.co_attention(img_features, que_features)\n",
        "        v_att = self.image_attention(v_att, q_att[:, -1, :])\n",
        "\n",
        "        combined = torch.cat([v_att, q_att[:, -1, :]], dim=1)\n",
        "        output = self.classifier(combined)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2k7-3A4rEMq"
      },
      "source": [
        "#### 学習の実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTaz643_rPMQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_vqa_score = 0\n",
        "    total_simple_acc = 0\n",
        "    start = time.time()\n",
        "    for batch in dataloader:\n",
        "        image, question, answers, mode_answer = [item.to(device) for item in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_vqa_score += vqa_score(pred, answers)\n",
        "        total_simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return (total_loss / num_batches,\n",
        "            total_vqa_score / num_batches,\n",
        "            total_simple_acc / num_batches,\n",
        "            time.time() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651OhxhCEJcs"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_vqa_score = 0\n",
        "    total_simple_acc = 0\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            image, question, answers, mode_answer = [item.to(device) for item in batch]\n",
        "\n",
        "            pred = model(image, question)\n",
        "            loss = criterion(pred, mode_answer)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_vqa_score += vqa_score(pred, answers)\n",
        "            total_simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return (total_loss / num_batches,\n",
        "            total_vqa_score / num_batches,\n",
        "            total_simple_acc / num_batches,\n",
        "            time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwWJxYfirR8H"
      },
      "source": [
        "#### 学習を実行(600秒:L4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "\n",
        "def main():\n",
        "    # ハイパーパラメータの設定\n",
        "    config = {\n",
        "        \"seed\": 42,\n",
        "        \"batch_size\": 128,\n",
        "        \"num_epochs\": 5,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"embed_size\": 300,\n",
        "        \"hidden_size\": 512,\n",
        "        \"image_size\": (224, 224),\n",
        "        \"max_question_length\": 12,\n",
        "        \"num_gru_layers\": 2,\n",
        "        \"data_path\": {\n",
        "            \"train_json\": \"/content/data/train.json\",\n",
        "            \"train_image\": \"/content/data/train\",\n",
        "            \"valid_json\": \"/content/data/valid.json\",\n",
        "            \"valid_image\": \"/content/data/valid\"\n",
        "        },\n",
        "        \"class_mapping_path\": \"/content/drive/MyDrive/DL_base/DL_base_Last/data/class_mapping.csv\"\n",
        "    }\n",
        "\n",
        "    # デバイスとシードの設定\n",
        "    set_seed(config[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # データの前処理\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(config[\"image_size\"]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # class_mappingの読み込み\n",
        "    class_mapping = load_class_mapping(config[\"class_mapping_path\"])\n",
        "\n",
        "    # データセットの準備\n",
        "    full_dataset = VQADataset(df_path=config[\"data_path\"][\"train_json\"],\n",
        "                              image_dir=config[\"data_path\"][\"train_image\"],\n",
        "                              transform=transform,\n",
        "                              max_length=config[\"max_question_length\"],\n",
        "                              class_mapping=class_mapping)\n",
        "\n",
        "    test_dataset = VQADataset(df_path=config[\"data_path\"][\"valid_json\"],\n",
        "                              image_dir=config[\"data_path\"][\"valid_image\"],\n",
        "                              transform=transform,\n",
        "                              max_length=config[\"max_question_length\"],\n",
        "                              answer=False,\n",
        "                              class_mapping=class_mapping)\n",
        "\n",
        "    # 5分割交差検証の設定\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=config[\"seed\"])\n",
        "\n",
        "    # 各foldの結果を保存するリスト\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"FOLD {fold}\")\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "        # データローダーの準備\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "        train_loader = DataLoader(full_dataset, batch_size=config[\"batch_size\"], sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_dataset, batch_size=config[\"batch_size\"], sampler=val_subsampler)\n",
        "\n",
        "        # モデルの初期化\n",
        "        model = VQAModel(vocab_size=len(full_dataset.question_vocab),\n",
        "                         embed_size=config[\"embed_size\"],\n",
        "                         hidden_size=config[\"hidden_size\"],\n",
        "                         num_classes=len(class_mapping),\n",
        "                         num_gru_layers=config[\"num_gru_layers\"]).to(device)\n",
        "\n",
        "        # GloVeの重みで埋め込み層を初期化\n",
        "        glove_embeddings = torch.zeros(len(full_dataset.question_vocab), config[\"embed_size\"])\n",
        "        for word, idx in full_dataset.question_vocab.items():\n",
        "            if word in full_dataset.glove.stoi:\n",
        "                glove_embeddings[idx] = full_dataset.glove.vectors[full_dataset.glove.stoi[word]]\n",
        "\n",
        "        model.question_encoder.embedding.weight.data.copy_(glove_embeddings)\n",
        "        model.question_encoder.embedding.weight.requires_grad = False\n",
        "\n",
        "        # 最適化器と損失関数の設定\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "        # 学習ループ\n",
        "        best_val_score = 0\n",
        "        for epoch in range(config[\"num_epochs\"]):\n",
        "            train_loss, train_vqa_score, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
        "            val_loss, val_vqa_score, val_simple_acc, val_time = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "            print(f\"Train - Loss: {train_loss:.4f}, VQA Score: {train_vqa_score:.4f}, Acc: {train_simple_acc:.4f}, Time: {train_time:.2f}s\")\n",
        "            print(f\"Val   - Loss: {val_loss:.4f}, VQA Score: {val_vqa_score:.4f}, Acc: {val_simple_acc:.4f}, Time: {val_time:.2f}s\")\n",
        "\n",
        "            # 最良モデルの保存\n",
        "            if val_vqa_score > best_val_score:\n",
        "                best_val_score = val_vqa_score\n",
        "                torch.save(model.state_dict(), f\"/content/drive/MyDrive/DL_base/DL_base_Last/data/best_model_fold{fold}.pth\")\n",
        "                print(\"Best model saved!\")\n",
        "\n",
        "        fold_results.append(best_val_score)\n",
        "        print(f\"Best validation score for fold {fold}: {best_val_score:.4f}\")\n",
        "\n",
        "    print(\"--------------------------------\")\n",
        "    print(f\"Average validation score: {np.mean(fold_results):.4f}\")\n",
        "    print(f\"Standard deviation: {np.std(fold_results):.4f}\")\n",
        "\n",
        "    # 全てのfoldで最も良かったモデルを使用してテストデータの予測\n",
        "    best_fold = np.argmax(fold_results)\n",
        "    best_model_path = f\"/content/drive/MyDrive/DL_base/DL_base_Last/data/best_model_fold{best_fold}.pth\"\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    submission = []\n",
        "    with torch.no_grad():\n",
        "        for image, question in test_loader:\n",
        "            image = image.to(device)\n",
        "            question = question.to(device)\n",
        "            pred = model(image, question)\n",
        "            pred = pred.argmax(1).cpu().item()\n",
        "            submission.append(full_dataset.idx2answer[pred])\n",
        "\n",
        "    submission = np.array(submission)\n",
        "\n",
        "    # 提出ファイルの保存\n",
        "    np.save(\"/content/drive/MyDrive/DL_base/DL_base_Last/data/submission_test.npy\", submission)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uncIhzjCYJ9K",
        "outputId": "11e5db8f-8fee-4c16-b754-308c097d6989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Train - Loss: 3.9657, VQA Score: 0.8297, Acc: 0.6319, Time: 483.66s\n",
            "Val   - Loss: 3.1922, VQA Score: 0.8400, Acc: 0.6400, Time: 99.18s\n",
            "Best model saved!\n",
            "Epoch 2/5\n",
            "Train - Loss: 3.1000, VQA Score: 0.8310, Acc: 0.6348, Time: 485.60s\n",
            "Val   - Loss: 3.2917, VQA Score: 0.8400, Acc: 0.6273, Time: 98.48s\n",
            "Best model saved!\n",
            "Epoch 3/5\n",
            "Train - Loss: 2.9297, VQA Score: 0.8324, Acc: 0.6346, Time: 482.80s\n",
            "Val   - Loss: 3.0433, VQA Score: 0.8383, Acc: 0.6402, Time: 97.94s\n",
            "Epoch 4/5\n",
            "Train - Loss: 2.8264, VQA Score: 0.8332, Acc: 0.6387, Time: 480.77s\n",
            "Val   - Loss: 3.0529, VQA Score: 0.8376, Acc: 0.6325, Time: 98.30s\n",
            "Epoch 5/5\n",
            "Train - Loss: 2.7568, VQA Score: 0.8337, Acc: 0.6392, Time: 485.26s\n",
            "Val   - Loss: 3.0510, VQA Score: 0.8313, Acc: 0.6293, Time: 101.12s\n",
            "Best validation score for fold 0: 0.8400\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Epoch 1/5\n",
            "Train - Loss: 7.5861, VQA Score: 0.8282, Acc: 0.6324, Time: 485.30s\n",
            "Val   - Loss: 3.2213, VQA Score: 0.8313, Acc: 0.6273, Time: 96.05s\n",
            "Best model saved!\n",
            "Epoch 2/5\n",
            "Train - Loss: 3.0421, VQA Score: 0.8305, Acc: 0.6340, Time: 485.77s\n",
            "Val   - Loss: 3.1047, VQA Score: 0.8441, Acc: 0.6402, Time: 96.79s\n",
            "Best model saved!\n",
            "Epoch 3/5\n",
            "Train - Loss: 2.8935, VQA Score: 0.8292, Acc: 0.6341, Time: 486.71s\n",
            "Val   - Loss: 3.0588, VQA Score: 0.8368, Acc: 0.6347, Time: 96.15s\n",
            "Epoch 4/5\n",
            "Train - Loss: 2.7984, VQA Score: 0.8338, Acc: 0.6400, Time: 483.41s\n",
            "Val   - Loss: 3.0969, VQA Score: 0.8372, Acc: 0.6335, Time: 95.89s\n",
            "Epoch 5/5\n",
            "Train - Loss: 2.7544, VQA Score: 0.8318, Acc: 0.6404, Time: 482.46s\n",
            "Val   - Loss: 3.1874, VQA Score: 0.8318, Acc: 0.6330, Time: 96.10s\n",
            "Best validation score for fold 1: 0.8441\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Epoch 1/5\n",
            "Train - Loss: 4.8939, VQA Score: 0.8298, Acc: 0.6319, Time: 483.15s\n",
            "Val   - Loss: 3.0429, VQA Score: 0.8344, Acc: 0.6405, Time: 97.65s\n",
            "Best model saved!\n",
            "Epoch 2/5\n",
            "Train - Loss: 3.0942, VQA Score: 0.8326, Acc: 0.6350, Time: 481.30s\n",
            "Val   - Loss: 3.1568, VQA Score: 0.8288, Acc: 0.6278, Time: 96.96s\n",
            "Epoch 3/5\n",
            "Train - Loss: 2.9545, VQA Score: 0.8338, Acc: 0.6366, Time: 479.38s\n",
            "Val   - Loss: 2.8785, VQA Score: 0.8415, Acc: 0.6447, Time: 96.98s\n",
            "Best model saved!\n",
            "Epoch 4/5\n",
            "Train - Loss: 2.8625, VQA Score: 0.8335, Acc: 0.6360, Time: 482.04s\n",
            "Val   - Loss: 2.9339, VQA Score: 0.8318, Acc: 0.6372, Time: 97.11s\n",
            "Epoch 5/5\n",
            "Train - Loss: 2.7881, VQA Score: 0.8349, Acc: 0.6373, Time: 482.08s\n",
            "Val   - Loss: 2.9311, VQA Score: 0.8335, Acc: 0.6399, Time: 97.46s\n",
            "Best validation score for fold 2: 0.8415\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Epoch 1/5\n",
            "Train - Loss: 3.9025, VQA Score: 0.8206, Acc: 0.6231, Time: 482.93s\n",
            "Val   - Loss: 3.0188, VQA Score: 0.8406, Acc: 0.6513, Time: 97.25s\n",
            "Best model saved!\n",
            "Epoch 2/5\n",
            "Train - Loss: 3.0564, VQA Score: 0.8326, Acc: 0.6335, Time: 481.77s\n",
            "Val   - Loss: 3.1123, VQA Score: 0.8424, Acc: 0.6453, Time: 96.52s\n",
            "Best model saved!\n",
            "Epoch 3/5\n",
            "Train - Loss: 2.9398, VQA Score: 0.8304, Acc: 0.6321, Time: 479.91s\n",
            "Val   - Loss: 2.9475, VQA Score: 0.8341, Acc: 0.6421, Time: 96.39s\n",
            "Epoch 4/5\n",
            "Train - Loss: 2.8353, VQA Score: 0.8349, Acc: 0.6373, Time: 480.23s\n",
            "Val   - Loss: 2.9546, VQA Score: 0.8334, Acc: 0.6431, Time: 97.32s\n",
            "Epoch 5/5\n",
            "Train - Loss: 2.7725, VQA Score: 0.8341, Acc: 0.6403, Time: 481.24s\n",
            "Val   - Loss: 2.9206, VQA Score: 0.8410, Acc: 0.6445, Time: 96.58s\n",
            "Best validation score for fold 3: 0.8424\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Epoch 1/5\n",
            "Train - Loss: 3.7336, VQA Score: 0.8318, Acc: 0.6343, Time: 485.14s\n",
            "Val   - Loss: 3.2371, VQA Score: 0.8361, Acc: 0.6366, Time: 95.33s\n",
            "Best model saved!\n",
            "Epoch 2/5\n",
            "Train - Loss: 3.0441, VQA Score: 0.8332, Acc: 0.6379, Time: 486.91s\n",
            "Val   - Loss: 3.4776, VQA Score: 0.8328, Acc: 0.6267, Time: 95.02s\n",
            "Epoch 3/5\n",
            "Train - Loss: 3.0129, VQA Score: 0.8306, Acc: 0.6356, Time: 491.33s\n",
            "Val   - Loss: 3.1122, VQA Score: 0.8356, Acc: 0.6364, Time: 94.57s\n",
            "Epoch 4/5\n",
            "Train - Loss: 2.8533, VQA Score: 0.8305, Acc: 0.6363, Time: 483.71s\n",
            "Val   - Loss: 3.0207, VQA Score: 0.8387, Acc: 0.6381, Time: 96.98s\n",
            "Best model saved!\n",
            "Epoch 5/5\n",
            "Train - Loss: 2.7672, VQA Score: 0.8330, Acc: 0.6398, Time: 483.31s\n",
            "Val   - Loss: 3.1065, VQA Score: 0.8391, Acc: 0.6321, Time: 94.15s\n",
            "Best model saved!\n",
            "Best validation score for fold 4: 0.8391\n",
            "--------------------------------\n",
            "Average validation score: 0.8414\n",
            "Standard deviation: 0.0018\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FGZ3jOaMs0S7",
        "OLKlq20GqEPC",
        "rzzRxglwqmUj",
        "qkSnUI11qzYF",
        "c2k7-3A4rEMq",
        "JAzTVKODDftC"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN4+/NRo+eJ34gCUaWNp6ax"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
